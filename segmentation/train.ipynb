{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from dataset import DRSegmentationDataset\n",
    "from unet import UNet\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "GPU = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:{GPU}\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DRSegmentationDataset(\"/home/wilk/diabetic_retinopathy/datasets/processed_segmentation_dataset/train_set\")\n",
    "test_dataset = DRSegmentationDataset(\"/home/wilk/diabetic_retinopathy/datasets/processed_segmentation_dataset/test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_level_train_dataset = DRSegmentationDataset(\"/home/wilk/diabetic_retinopathy/datasets/grading/train_set\", use_masks=False)\n",
    "image_level_test_dataset = DRSegmentationDataset(\"/home/wilk/diabetic_retinopathy/datasets/grading/test_set\", use_masks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"runs/Adam_original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Fold training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    # print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, epochs, k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True) \n",
    "    fold_results = {fold: 0 for fold in range(k_folds)}\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "                      train_dataset, \n",
    "                      batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "                      train_dataset,\n",
    "                      batch_size=BATCH_SIZE, sampler=val_subsampler)\n",
    "        \n",
    "        model = UNet(in_channels=3, out_channels=5)\n",
    "        model.to(device)\n",
    "\n",
    "        model.apply(reset_weights)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, betas=[0.5, 0.5])\n",
    "\n",
    "        loss = torch.nn.BCELoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            training_epoch_loss = 0\n",
    "            validation_epoch_loss = 0\n",
    "\n",
    "            model.train()\n",
    "            for train_batch_id, train_batch in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                input_tensor = train_batch[0].to(device)\n",
    "                target_tensor = train_batch[1].to(device)\n",
    "\n",
    "                train_output = model(input_tensor)\n",
    "\n",
    "                loss_value = loss(train_output, target_tensor)\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                training_epoch_loss += loss_value.item()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_batch_id, val_batch in enumerate(val_loader):                \n",
    "                    input_tensor = val_batch[0].to(device)\n",
    "                    target_tensor = val_batch[1].to(device)\n",
    "\n",
    "                    val_output = model(input_tensor)\n",
    "\n",
    "                    loss_value = loss(val_output, target_tensor)\n",
    "                    validation_epoch_loss += loss_value.item() \n",
    "\n",
    "            training_epoch_loss /= len(train_loader)\n",
    "            validation_epoch_loss /= len(val_loader)\n",
    "\n",
    "            writer.add_scalar(f'Fold{fold}/Loss/Train', training_epoch_loss, epoch)\n",
    "            writer.add_scalar(f'Fold{fold}/Loss/Val', validation_epoch_loss, epoch)\n",
    "\n",
    "\n",
    "            print(f\"Fold: {fold}, Epoch: {epoch}, Mean training loss: {training_epoch_loss}, Mean validation loss: {validation_epoch_loss}\")\n",
    "        fold_results[fold] = validation_epoch_loss\n",
    "    \n",
    "    final_fold_values = [value for k, value in fold_results.items()]\n",
    "    average_validation_result = np.mean(final_fold_values)\n",
    "    print(\"Average validation result:\", average_validation_result)\n",
    "\n",
    "    writer.add_scalar(\"Average validation result:\", average_validation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(train_dataset, 200, k_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final training with Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_validation(validation_px_dataloader, validation_image_level_dataloader, generator_model, discriminator_model):\n",
    "    validation_accuracy = []\n",
    "\n",
    "    targets = []\n",
    "    predicted_values = []\n",
    "\n",
    "    generator_model.eval()\n",
    "    discriminator_model.eval()\n",
    "    for px_level_batch, img_level_batch in zip(validation_px_dataloader, validation_image_level_dataloader):\n",
    "        \n",
    "        px_level_input_tensor = px_level_batch[0].to(device)\n",
    "        img_level_input_tensor = img_level_batch[0].to(device)\n",
    "\n",
    "        # Generator masks for px level dataset and image level dataset\n",
    "        px_level_output = generator_model(px_level_input_tensor)\n",
    "        img_level_output = generator_model(img_level_input_tensor)\n",
    "\n",
    "        px_level_output = torch.concat((px_level_output, px_level_input_tensor), dim=1) # Stack with original RGB image\n",
    "        img_level_output = torch.concat((img_level_output, img_level_input_tensor), dim=1) # Stack with original RGB image\n",
    "\n",
    "        px_level_target = torch.Tensor([[1] for i in range(px_level_output.shape[0])])\n",
    "        img_level_target = torch.Tensor([[0] for i in range(img_level_output.shape[0])])\n",
    "\n",
    "        discriminator_input = torch.concat((px_level_output, img_level_output), dim=0)\n",
    "        discriminator_target = torch.concat((px_level_target, img_level_target), dim=0)\n",
    "\n",
    "        indices = torch.randperm(discriminator_input.shape[0])\n",
    "        discriminator_input = discriminator_input[indices, ...].to(device)\n",
    "        discriminator_target = discriminator_target[indices, ...].to(device)\n",
    "\n",
    "        discriminator_output = discriminator_model(discriminator_input)\n",
    "\n",
    "        discriminator_output = torch.where(discriminator_output > 0.5, 1, 0)\n",
    "\n",
    "        batch_sum = torch.sum(discriminator_output == discriminator_target) \n",
    "        batch_accuracy = batch_sum / discriminator_output.size()[0]\n",
    "        validation_accuracy.append(batch_accuracy)\n",
    "\n",
    "        targets += discriminator_target.squeeze().cpu().detach().tolist()\n",
    "        predicted_values += discriminator_output.squeeze().cpu().detach().tolist()\n",
    "\n",
    "    targets = np.array(targets)\n",
    "    predicted_values = np.array(predicted_values)\n",
    "\n",
    "    accuracy = np.mean(targets == predicted_values)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, image_level_dataloader, epochs, model, discriminator_model):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, betas=[0.5, 0.5])\n",
    "        discriminator_optimizer = torch.optim.Adam(discriminator_model.parameters(), lr=1e-4)\n",
    "\n",
    "        loss = torch.nn.BCELoss()\n",
    "        discriminator_loss = torch.nn.BCELoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            training_epoch_loss = 0\n",
    "\n",
    "            model.train()\n",
    "            for train_batch_id, train_batch in enumerate(train_dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                input_tensor = train_batch[0].to(device)\n",
    "                target_tensor = train_batch[1].to(device)\n",
    "\n",
    "                train_output = model(input_tensor)\n",
    "\n",
    "                loss_value = loss(train_output, target_tensor)\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                training_epoch_loss += loss_value.item()\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            discriminator_epoch_loss = 0\n",
    "\n",
    "            model.eval()\n",
    "            discriminator_model.train()\n",
    "            for px_level_batch, img_level_batch in zip(train_dataloader, image_level_dataloader):\n",
    "                \n",
    "                px_level_input_tensor = px_level_batch[0].to(device)\n",
    "                img_level_input_tensor = img_level_batch[0].to(device)\n",
    "\n",
    "                # Generator masks for px level dataset and image level dataset\n",
    "                px_level_output = model(px_level_input_tensor)\n",
    "                img_level_output = model(img_level_input_tensor)\n",
    "\n",
    "                px_level_output = torch.concat((px_level_output, px_level_input_tensor), dim=1) # Stack with original RGB image\n",
    "                img_level_output = torch.concat((img_level_output, img_level_input_tensor), dim=1) # Stack with original RGB image\n",
    "\n",
    "                px_level_target = torch.Tensor([[1] for i in range(px_level_output.shape[0])])\n",
    "                img_level_target = torch.Tensor([[0] for i in range(img_level_output.shape[0])])\n",
    "\n",
    "                discriminator_input = torch.concat((px_level_output, img_level_output), dim=0)\n",
    "                discriminator_target = torch.concat((px_level_target, img_level_target), dim=0)\n",
    "\n",
    "                indices = torch.randperm(discriminator_input.shape[0])\n",
    "                discriminator_input=discriminator_input[indices, ...].to(device)\n",
    "                discriminator_target=discriminator_target[indices, ...].to(device)\n",
    "\n",
    "                discriminator_output = discriminator_model(discriminator_input)\n",
    "\n",
    "                discriminator_loss_value = discriminator_loss(discriminator_output, discriminator_target)\n",
    "\n",
    "                discriminator_loss_value.backward()\n",
    "                discriminator_optimizer.step()\n",
    "                discriminator_optimizer.zero_grad()\n",
    "\n",
    "                discriminator_epoch_loss += discriminator_loss_value\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            accuracy = discriminator_validation(train_dataloader, image_level_dataloader, model, discriminator_model)\n",
    "\n",
    "            mean_discriminator_loss = discriminator_epoch_loss / len(train_dataloader)\n",
    "\n",
    "            training_epoch_loss /= len(train_dataloader)\n",
    "\n",
    "            writer.add_scalar('Final/Loss/Train', training_epoch_loss, epoch)\n",
    "            writer.add_scalar('Final/Discriminator_loss/Train', mean_discriminator_loss, epoch)\n",
    "\n",
    "            print(f\"Epoch: {epoch}, Mean training loss: {training_epoch_loss}, Mean discriminator loss: {mean_discriminator_loss}, Discriminator accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "                      train_dataset, \n",
    "                      batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                      test_dataset, \n",
    "                      batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_level_train_dataloader = torch.utils.data.DataLoader(image_level_train_dataset, batch_size=BATCH_SIZE)\n",
    "image_level_test_dataloader = torch.utils.data.DataLoader(image_level_test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3, 5)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model = Discriminator()\n",
    "discriminator_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataloader, image_level_train_dataloader, 100, model, discriminator_model=discriminator_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"segmentation_generator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = UNet(3, 5)\n",
    "loaded_model.load_state_dict(torch.load(\"segmentation_generator.pth\", map_location=device))\n",
    "loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "loss = torch.nn.BCELoss()\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_batch_id, test_batch in enumerate(test_dataloader):                \n",
    "        input_tensor = test_batch[0].to(device)\n",
    "        target_tensor = test_batch[1].to(device)\n",
    "\n",
    "        val_output = loaded_model(input_tensor)\n",
    "\n",
    "        loss_value = loss(val_output, target_tensor)\n",
    "        test_loss += loss_value.item() \n",
    "\n",
    "mean_test_loss = test_loss / len(test_dataloader)\n",
    "print(\"Mean test loss:\", mean_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_batch_id, test_batch in enumerate(test_dataloader):                \n",
    "    input_tensor = test_batch[0].to(device)\n",
    "    target_tensor = test_batch[1].to(device)\n",
    "\n",
    "    test_output = loaded_model(input_tensor)\n",
    "\n",
    "    target_tensor = target_tensor.squeeze()\n",
    "    test_output = test_output.squeeze()\n",
    "\n",
    "\n",
    "    # fig = plt.figure()\n",
    "    # for i in range(5):\n",
    "    #     ax = fig.add_subplot(2,5,i+1)\n",
    "    #     ax.imshow(target_tensor[i, ...].cpu(), cmap='gray')\n",
    "    #     ax.set_title(\"Ground truth\")\n",
    "\n",
    "    # for i in range(5):\n",
    "    #     ax = fig.add_subplot(2,5, i+5+1)\n",
    "    #     ax.imshow(test_output[i, ...].cpu().detach(), cmap='gray')\n",
    "    #     ax.set_title(\"Prdicted\")\n",
    "\n",
    "    lesion_index = 4\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.subplots_adjust(bottom=0.1, right=0.8, top=2)\n",
    "\n",
    "    ax = fig.add_subplot(2,1,1)\n",
    "    ax.imshow(target_tensor[lesion_index, ...].cpu(), cmap='gray')\n",
    "    ax.set_title(\"Ground truth\")\n",
    "\n",
    "    ax = fig.add_subplot(2,1, 2)\n",
    "    ax.imshow(test_output[lesion_index, ...].cpu().detach(), cmap='gray')\n",
    "    ax.set_title(\"Prdicted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
