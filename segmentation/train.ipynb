{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from dataset import DRSegmentationDataset\n",
    "from unet import UNet\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DRSegmentationDataset(\"/home/wilk/diabetic_retinopathy/datasets/processed_segmentation_dataset/test_set\")\n",
    "train_dataset = DRSegmentationDataset(\"/home/wilk/diabetic_retinopathy/datasets/processed_segmentation_dataset/train_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"runs/Adam_original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    # print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, epochs, k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True) \n",
    "    fold_results = {fold: 0 for fold in range(k_folds)}\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "                      train_dataset, \n",
    "                      batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "                      train_dataset,\n",
    "                      batch_size=BATCH_SIZE, sampler=val_subsampler)\n",
    "        \n",
    "        model = UNet(in_channels=3, out_channels=5)\n",
    "        model.to(device)\n",
    "\n",
    "        model.apply(reset_weights)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, betas=[0.5, 0.5])\n",
    "\n",
    "        loss = torch.nn.BCELoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            training_epoch_loss = 0\n",
    "            validation_epoch_loss = 0\n",
    "\n",
    "            model.train()\n",
    "            for train_batch_id, train_batch in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                input_tensor = train_batch[0].to(device)\n",
    "                target_tensor = train_batch[1].to(device)\n",
    "\n",
    "                train_output = model(input_tensor)\n",
    "\n",
    "                loss_value = loss(train_output, target_tensor)\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                training_epoch_loss += loss_value.item()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_batch_id, val_batch in enumerate(val_loader):                \n",
    "                    input_tensor = val_batch[0].to(device)\n",
    "                    target_tensor = val_batch[1].to(device)\n",
    "\n",
    "                    val_output = model(input_tensor)\n",
    "\n",
    "                    loss_value = loss(val_output, target_tensor)\n",
    "                    validation_epoch_loss += loss_value.item() \n",
    "\n",
    "            training_epoch_loss /= len(train_loader)\n",
    "            validation_epoch_loss /= len(val_loader)\n",
    "\n",
    "            writer.add_scalar(f'Fold{fold}/Loss/Train', training_epoch_loss, epoch)\n",
    "            writer.add_scalar(f'Fold{fold}/Loss/Val', validation_epoch_loss, epoch)\n",
    "\n",
    "\n",
    "            print(f\"Fold: {fold}, Epoch: {epoch}, Mean training loss: {training_epoch_loss}, Mean validation loss: {validation_epoch_loss}\")\n",
    "        fold_results[fold] = validation_epoch_loss\n",
    "    \n",
    "    final_fold_values = [value for k, value in fold_results.items()]\n",
    "    average_validation_result = np.mean(final_fold_values)\n",
    "    print(\"Average validation result:\", average_validation_result)\n",
    "\n",
    "    writer.add_scalar(\"Average validation result:\", average_validation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, 200, k_folds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
